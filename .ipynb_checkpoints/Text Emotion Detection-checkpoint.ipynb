{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 8706\n"
     ]
    }
   ],
   "source": [
    "# (1) preprocessing - reading the dataset\n",
    "# Function for reading the dataset file\n",
    "def read_data(file):\n",
    "    data = []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
    "            text = line[line.find(\"]\")+1:].strip()\n",
    "            data.append([label, text])\n",
    "    return data\n",
    "\n",
    "# File name\n",
    "file = \"c:\\\\Users\\Sai raj\\Desktop\\My Project\\dataprep.txt\"\n",
    "data = read_data(file)\n",
    "\n",
    "print(f\"Number of instances: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sai\n",
      "[nltk_data]     raj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# (2) preprocessing - removing the stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Function for removing the stop word from the text\n",
    "def remove_stop_words(data):\n",
    "    sentences = \"\"\n",
    "    data = data.split('\\n')\n",
    "    #print(data)\n",
    "    for text in data:\n",
    "        # Replacing each special character and numbers with a space\n",
    "        text_alphanum = re.sub('[^a-z]', ' ', text)\n",
    "        word_tokens = word_tokenize(text_alphanum)\n",
    "        \n",
    "        # Removing stop words\n",
    "        sentence = ' '.join([w for w in word_tokens if (w not in stop_words)])\n",
    "        sentences += sentence + \"\\n\"\n",
    "        #print(sentence)\n",
    "        \n",
    "    return sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sai\n",
      "[nltk_data]     raj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# (3) preprocessing - text normalization using lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# word lemmatization (Normalization)\n",
    "def noun_lemmatizer(sentences):\n",
    "    # Init the Wordnet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentences = sentences.split('\\n')\n",
    "    #print(sentences)\n",
    "    lem_text = ''\n",
    "    for line in sentences:\n",
    "        #print(line)\n",
    "        word_tokens = word_tokenize(line)\n",
    "        sentence = ' '.join([lemmatizer.lemmatize(w, 'v') for w in word_tokens])\n",
    "        lem_text += sentence + '\\n'\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating ngrams of words \n",
    "def ngram(token, n):\n",
    "    output = []\n",
    "    for i in range(n-1, len(token)):\n",
    "        ngram = ' '.join(token[i-n+1:i+1])\n",
    "        output.append(ngram)\n",
    "        #print(output)\n",
    "    return output\n",
    "\n",
    "# Function for creating feature\n",
    "def create_feature(text, nrange=(1,1)):\n",
    "    text_features = []\n",
    "    text = text.lower()\n",
    "    ###################\n",
    "    #print(text)\n",
    "    text = remove_stop_words(text)\n",
    "    #print(text)\n",
    "    text = noun_lemmatizer(text)\n",
    "    #print(text)\n",
    "    ###################\n",
    "    text_alphanum = text\n",
    "    #print(text_alphanum)\n",
    "    for n in range(nrange[0], nrange[1]+1):\n",
    "        text_features += ngram(text_alphanum.split(), n)\n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    #print(text_punc)\n",
    "    text_features += ngram(text_punc.split(), 1)\n",
    "    #print(Counter(text_features))\n",
    "    return Counter(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(item, name):\n",
    "    #print(item)\n",
    "    items = list(map(float, item.split()))\n",
    "    label = \"\"\n",
    "    for idx in range(len(items)):\n",
    "        if items[idx] == 1:\n",
    "            label += name[idx] + \" \"\n",
    "    return label.strip()\n",
    "\n",
    "emotions = [\"joy\", \"fear\", \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "\n",
    "X_all = []\n",
    "Y_all = []\n",
    "for label, text in data:\n",
    "    Y_all.append(convert_label(label, emotions))\n",
    "    X_all.append(create_feature(text, nrange=(1, 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all, Y_all, test_size = 0.2, random_state = 123)\n",
    "\n",
    "def train_test(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    return train_acc, test_acc\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer(sparse = True)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Classifier                | Training Accuracy | Test Accuracy |\n",
      "| ------------------------- | ----------------- | ------------- |\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to SVC.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5a47d9c43145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mclf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"| {:25} SVM | {:17.7f} | {:13.7f} |\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to SVC.__format__"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "linear_svm = SVC(kernel='linear')\n",
    "rbf_svm = SVC(kernel ='rbf', random_state = 0)\n",
    "poly_svm = SVC(kernel='poly', degree=8)\n",
    "sigmoid_svm = SVC(kernel ='sigmoid')\n",
    "\n",
    "clifs = [linear_svm, rbf_svm, poly_svm, sigmoid_svm]\n",
    "\n",
    "# train and test them \n",
    "print(\"| {:25} | {} | {} |\".format(\"Classifier\", \"Training Accuracy\", \"Test Accuracy\"))\n",
    "print(\"| {} | {} | {} |\".format(\"-\"*25, \"-\"*17, \"-\"*13))\n",
    "for clf in clifs: \n",
    "    clf_name = clf\n",
    "    train_acc, test_acc = train_test(clf, X_train, X_test, y_train, y_test)\n",
    "    print(\"| {:25} SVM | {:17.7f} | {:13.7f} |\".format(clf_name, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy       (1. 0. 0. 0. 0. 0. 0.)  1084\n",
      "anger     (0. 0. 1. 0. 0. 0. 0.)  1080\n",
      "sadness   (0. 0. 0. 1. 0. 0. 0.)  1079\n",
      "fear      (0. 1. 0. 0. 0. 0. 0.)  1078\n",
      "disgust   (0. 0. 0. 0. 1. 0. 0.)  1057\n",
      "guilt     (0. 0. 0. 0. 0. 0. 1.)  1057\n",
      "shame     (0. 0. 0. 0. 0. 1. 0.)  1045\n"
     ]
    }
   ],
   "source": [
    "l = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "l.sort()\n",
    "label_freq = {}\n",
    "for label, _ in data: \n",
    "    label_freq[label] = label_freq.get(label, 0) + 1\n",
    "\n",
    "# print the labels and their counts in sorted order \n",
    "for l in sorted(label_freq, key=label_freq.get, reverse=True):\n",
    "    print(\"{:10}({})  {}\".format(convert_label(l, emotions), l, label_freq[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love much\n",
      "\n",
      "Counter({'love': 1, 'much': 1, 'love much': 1})\n",
      "This love you so much ðŸ˜‚\n",
      "speak english becuase poor english\n",
      "\n",
      "Counter({'english': 2, 'speak': 1, 'becuase': 1, 'poor': 1, 'speak english': 1, 'english becuase': 1, 'becuase poor': 1, 'poor english': 1, 'speak english becuase': 1, 'english becuase poor': 1, 'becuase poor english': 1, 'speak english becuase poor': 1, 'english becuase poor english': 1})\n",
      "I can't speak english becuase I'm poor in english ðŸ˜¢\n",
      "awsome\n",
      "\n",
      "Counter({'awsome': 1})\n",
      "you are awsome ðŸ˜³\n",
      "love anymore\n",
      "\n",
      "Counter({'love': 1, 'anymore': 1, 'love anymore': 1})\n",
      "I love you anymore..! ðŸ˜‚\n"
     ]
    }
   ],
   "source": [
    "emoji_dict = {\"joy\":\"ðŸ˜‚\", \"fear\":\"ðŸ˜±\", \"anger\":\"ðŸ˜ \", \"sadness\":\"ðŸ˜¢\", \"disgust\":\"ðŸ˜’\", \"shame\":\"ðŸ˜³\", \"guilt\":\"ðŸ˜³\"}\n",
    "t1 = \"This love you so much\"\n",
    "t2 = \"I can't speak english becuase I'm poor in english\"\n",
    "t3 = \"you are awsome\"\n",
    "t4 = \"I love you anymore..!\"\n",
    "\n",
    "texts = [t1, t2, t3, t4]\n",
    "for text in texts: \n",
    "    features = create_feature(text, nrange=(1, 4))\n",
    "    features = vectorizer.transform(features)\n",
    "    prediction = clf.predict(features)[0]\n",
    "    print( text,emoji_dict[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
