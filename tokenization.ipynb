{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    data = []\n",
    "    with open(file_name, 'r') as fp:\n",
    "        for line in fp:\n",
    "            label = ' '.join(line[1: line.find(']')].strip().split())\n",
    "            text = line[line.find(']')+1: ].strip()\n",
    "            data.append([label, text])\n",
    "        #print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 5\n"
     ]
    }
   ],
   "source": [
    "file_name = \"c:\\\\users\\sai raj\\desktop\\my project\\dataset1.txt\"\n",
    "data = read_data(file_name)\n",
    "print(f\"Number of instances: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period falling love time met especially met long time\n",
      "involved traffic accident\n",
      "driving home several days hard work motorist ahead driving hour refused despite low speeed let overtake\n",
      "lost person meant\n",
      "time knocked deer sight animal injuries helplessness realization animal badly hurt put animal screamed moment death\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sai\n",
      "[nltk_data]     raj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(data)\n",
    "    sentences = \"\"\n",
    "    for label, text in data:\n",
    "        #print(label, text)\n",
    "        text = text.lower()\n",
    "        # Replacing each special character and numbers with a space\n",
    "        text_alphanum = re.sub('[^a-z]', ' ', text)\n",
    "        word_tokens = word_tokenize(text_alphanum)\n",
    "        sentence = ' '.join([w for w in word_tokens if (w not in stop_words) and len(w) >2 ])\n",
    "        sentences += sentence + \"\\n\"\n",
    "        #print(sentence)\n",
    "return sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sai\n",
      "[nltk_data]     raj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def noun_lemmatizer(sentences):\n",
    "    # Init the Wordnet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentences = list(sentences)\n",
    "    lem_text = ''\n",
    "    for line in sentences:\n",
    "        print(line)\n",
    "        word_tokens = word_tokenize(text_alphanum)\n",
    "        sentence = ' '.join([lemmatizer.lemmatize(w) for w in word_tokens])\n",
    "        lem_text += sentence + '\\n'\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
